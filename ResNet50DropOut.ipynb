{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "Lets start with importing some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:51:22.057635900Z",
     "start_time": "2023-06-13T13:51:21.997253500Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras as K\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "K.utils.set_random_seed(270219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with two functions: One to get the ID of each image and one to find the index of each image of the training set. In this function, we have some folder named data containing the images and the csv file. We store all the images in the variable images, the IDs are the IDs of the images (the number after img_) and labels contain the id with the respective labels. trainIndex then is the images ID that correspond to the labels.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:39:14.997620400Z",
     "start_time": "2023-06-13T13:39:14.949169400Z"
    }
   },
   "outputs": [],
   "source": [
    "def getImageID(imageList):\n",
    "    ID = []\n",
    "    for i in imageList:\n",
    "        ID.append(re.findall(\"\\d{1,}\", i))\n",
    "    return ID\n",
    "\n",
    "def getIndex(IDs,labelsId):\n",
    "   trainIndex = []\n",
    "   for labeled in labelsId:\n",
    "       for IDindex, ID in enumerate(IDs):\n",
    "           if str(labeled) == ID[0]:\n",
    "               trainIndex.append(IDindex)\n",
    "   return trainIndex\n",
    "\n",
    "def getValidationIndex(IDs, labelsID):\n",
    "    validationIndex = []\n",
    "    for IDindex, ID in enumerate(IDs):\n",
    "        validation = 1\n",
    "        for label in labelsID:\n",
    "            if str(label) == ID[0]:\n",
    "                validation = 0\n",
    "        if validation:\n",
    "            validationIndex.append(IDindex)\n",
    "    return validationIndex\n",
    "localPath = os.getcwd()\n",
    "dataPath = localPath +\"\\data\\\\\"\n",
    "\n",
    "images = [f for f in listdir(dataPath) if isfile(join(dataPath, f))]\n",
    "del images[-1]\n",
    "\n",
    "IDs = getImageID(images)\n",
    "labels = pd.read_csv(dataPath+\"\\labels.csv\")\n",
    "labels.malignant = labels.malignant+1\n",
    "trainIndex = getIndex(IDs,labels.id)\n",
    "validationIndex = getValidationIndex(IDs, labels.id)\n",
    "validationID =[]\n",
    "for index in validationIndex:\n",
    "    validationID.append(IDs[index][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have the images. Not all images are the same size however, lets find the maximum image size so we know how much we need to pad the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:40:47.928224200Z",
     "start_time": "2023-06-13T12:40:47.897851900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum image size will be: [896, 896]\n"
     ]
    }
   ],
   "source": [
    "imageSize = [0,0]\n",
    "for image in images:\n",
    "    im = Image.open(dataPath+image)\n",
    "    if imageSize[0] < im.size[0]:\n",
    "        imageSize[0] = im.size[0]\n",
    "    if imageSize[1] < im.size[1]:\n",
    "        imageSize[1] = im.size[1]\n",
    "print(\"Maximum image size will be: \"+ str(imageSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what the maximum size will be, lets pad all the images to our desired dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:40:52.954586700Z",
     "start_time": "2023-06-13T12:40:52.931115Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_margin(pil_img, top, right):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right\n",
    "    new_height = height + top\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), (0, 0, 0))\n",
    "    result.paste(pil_img, (0, top))\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    if not len(listdir(\"data_padded\")) == 186:\n",
    "        for image in images:\n",
    "            im = Image.open(dataPath+image)\n",
    "            dWidth =  imageSize[0] - im.size[0]\n",
    "            dHeight = imageSize[1] - im.size[1]\n",
    "            if dWidth or dHeight:\n",
    "                im = add_margin(im, dHeight,dWidth)\n",
    "            im.save('data_padded/'+image)\n",
    "except:\n",
    "    for image in images:\n",
    "        im = Image.open(dataPath+image)\n",
    "    dWidth =  imageSize[0].astype(int) - im.size[0]\n",
    "    dHeight = imageSize[1].astype(int) - im.size[1]\n",
    "    if dWidth or dHeight:\n",
    "        im = add_margin(im, dHeight,dWidth)\n",
    "    im.save('data_padded/'+image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:23:53.237958300Z",
     "start_time": "2023-06-13T13:23:47.700398800Z"
    }
   },
   "outputs": [],
   "source": [
    "paddedImages = [f for f in listdir(\"data_padded\") if isfile(join(\"data_padded\", f))]\n",
    "trainData = list(K.utils.img_to_array(Image.open('data_padded/'+paddedImages[i])) for i in trainIndex)\n",
    "\n",
    "for i in range(len(trainData)):\n",
    "    trainData[i] = (trainData[i]-np.mean(trainData[i]))/np.std(trainData[i])\n",
    "\n",
    "validationData = list(K.utils.img_to_array(Image.open('data_padded/'+paddedImages[i])) for i in validationIndex)\n",
    "\n",
    "for i in range(len(validationData)):\n",
    "    validationData[i] = (validationData[i]-np.mean(validationData[i]))/np.std(validationData[i])\n",
    "\n",
    "trainData = np.array(trainData)\n",
    "validationData =np.array(validationData)\n",
    "labelData = np.array(labels.malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:11:32.149169Z",
     "start_time": "2023-06-13T14:11:30.320333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 resnet50 - False\n",
      "1 dropout_3 - True\n",
      "2 flatten_6 - True\n",
      "3 dropout_4 - True\n",
      "4 dense_6 - True\n"
     ]
    }
   ],
   "source": [
    "resmodel = applications.ResNet50(include_top=False, weights='imagenet', input_shape=(imageSize[0], imageSize[1], 3))\n",
    "\n",
    "model = K.models.Sequential()\n",
    "model.add(resmodel)\n",
    "model.add(K.layers.Dropout(.2, input_shape=(7,7,2048)))\n",
    "model.add(K.layers.Flatten())\n",
    "model.add(K.layers.Dropout(.2, input_shape=(7,7,2048)))\n",
    "model.add(K.layers.Dense(3, activation='softmax',\n",
    "                         kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                         bias_regularizer=regularizers.L2(1e-4),))\n",
    "for layer in model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, \"-\", layer.trainable)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n",
    "modelName = \"DoubleDropout5Epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:15:31.930151500Z",
     "start_time": "2023-06-13T14:11:33.668453800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 47s 11s/step - loss: 30.6918 - accuracy: 0.4677\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 44s 11s/step - loss: 21.4744 - accuracy: 0.5323\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 45s 11s/step - loss: 11.6409 - accuracy: 0.5645\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 42s 10s/step - loss: 7.0293 - accuracy: 0.6935\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 42s 10s/step - loss: 6.2937 - accuracy: 0.7581\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 28, 28, 2048)      23587712  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 28, 28, 2048)      0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 1605632)           0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1605632)           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 4816899   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,404,611\n",
      "Trainable params: 4,816,899\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DoubleDropout5Epoch\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DoubleDropout5Epoch\\assets\n"
     ]
    }
   ],
   "source": [
    "#startpoint 10 epochs\n",
    "if not os.path.isdir(modelName):\n",
    "    history = model.fit(trainData,labelData , verbose = 1, batch_size = 16, epochs = 5)\n",
    "    model.summary()\n",
    "    model.save(modelName)\n",
    "else:\n",
    "    print(\"Model already exist, not retraining it :)\")\n",
    "    answer = input(\"Do you want to load the model instead? Y/N\")\n",
    "    if answer == \"Y\" or answer == \"y\":\n",
    "        model = K.models.load_model(modelName)\n",
    "        print(\"Model successfully loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 89s 22s/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(validationData)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:09.200410400Z",
     "start_time": "2023-06-13T14:15:39.939125200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  malignant\n",
      "0    103601         -1\n",
      "1    105480         -1\n",
      "2    118847         -1\n",
      "3    125877          1\n",
      "4    133778         -1\n",
      "..      ...        ...\n",
      "119  968389         -1\n",
      "120   97549         -1\n",
      "121  976505          1\n",
      "122  996288         -1\n",
      "123  997841         -1\n",
      "\n",
      "[124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "predictedType = []\n",
    "for p in prediction:\n",
    "    maxp = np.argmax(p)\n",
    "    predictedType.append(maxp-1)\n",
    "\n",
    "predictions = {\"id\": validationID, \"malignant\": predictedType}\n",
    "predictiondf = pd.DataFrame(data = predictions)\n",
    "predictiondf.to_csv(\"prediction.csv\", index= False)\n",
    "print(predictiondf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:17.185568700Z",
     "start_time": "2023-06-13T14:17:17.169953800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "#in advance, maybe lets try different models?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T14:06:47.046268500Z",
     "start_time": "2023-06-13T14:06:47.029114100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
