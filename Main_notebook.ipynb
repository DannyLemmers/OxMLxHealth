{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "Lets start with importing some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:32:44.810022600Z",
     "start_time": "2023-06-11T20:32:44.799521200Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "import tensorflow.keras as K\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "K.utils.set_random_seed(270219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with two functions: One to get the ID of each image and one to find the index of each image of the training set. In this function, we have some folder named data containing the images and the csv file. We store all the images in the variable images, the IDs are the IDs of the images (the number after img_) and labels contain the id with the respective labels. trainIndex then is the images ID that correspond to the labels.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:32:50.760394600Z",
     "start_time": "2023-06-11T20:32:50.738707100Z"
    }
   },
   "outputs": [],
   "source": [
    "def getImageID(imageList):\n",
    "    ID = []\n",
    "    for i in imageList:\n",
    "        ID.append(re.findall(\"\\d{1,}\", i))\n",
    "    return ID\n",
    "\n",
    "def getTrainIndex(IDs,labelsId):\n",
    "   index = []\n",
    "   for labeled in labelsId:\n",
    "       for IDindex, ID in enumerate(IDs):\n",
    "           if str(labeled) == ID[0]:\n",
    "               index.append(IDindex)\n",
    "   return index\n",
    "\n",
    "localPath = os.getcwd()\n",
    "dataPath = localPath +\"\\data\\\\\"\n",
    "\n",
    "images = [f for f in listdir(dataPath) if isfile(join(dataPath, f))]\n",
    "del images[-1]\n",
    "\n",
    "IDs = getImageID(images)\n",
    "labels = pd.read_csv(dataPath+\"\\labels.csv\")\n",
    "labels.malignant = labels.malignant+1\n",
    "trainIndex = getTrainIndex(IDs,labels.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have the images. Not all images are the same size however, lets find the maximum image size so we know how much we need to pad the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:32:52.525659400Z",
     "start_time": "2023-06-11T20:32:52.485543500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum image size will be: [896, 896]\n"
     ]
    }
   ],
   "source": [
    "imageSize = [0,0]\n",
    "for image in images:\n",
    "    im = Image.open(dataPath+image)\n",
    "    if imageSize[0] < im.size[0]:\n",
    "        imageSize[0] = im.size[0]\n",
    "    if imageSize[1] < im.size[1]:\n",
    "        imageSize[1] = im.size[1]\n",
    "print(\"Maximum image size will be: \"+ str(imageSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what the maximum size will be, lets pad all the images to our desired dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:32:54.344938600Z",
     "start_time": "2023-06-11T20:32:54.302871600Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_margin(pil_img, top, right):\n",
    "    width, height = pil_img.size\n",
    "    new_width = width + right\n",
    "    new_height = height + top\n",
    "    result = Image.new(pil_img.mode, (new_width, new_height), (0, 0, 0))\n",
    "    result.paste(pil_img, (0, top))\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    if not len(listdir(\"data_padded\")) == 186:\n",
    "        for image in images:\n",
    "            im = Image.open(dataPath+image)\n",
    "            dWidth =  imageSize[0] - im.size[0]\n",
    "            dHeight = imageSize[1] - im.size[1]\n",
    "            if dWidth or dHeight:\n",
    "                im = add_margin(im, dHeight,dWidth)\n",
    "            im.save('data_padded/'+image)\n",
    "except:\n",
    "    for image in images:\n",
    "        im = Image.open(dataPath+image)\n",
    "    dWidth =  imageSize[0].astype(int) - im.size[0]\n",
    "    dHeight = imageSize[1].astype(int) - im.size[1]\n",
    "    if dWidth or dHeight:\n",
    "        im = add_margin(im, dHeight,dWidth)\n",
    "    im.save('data_padded/'+image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:32:57.776541100Z",
     "start_time": "2023-06-11T20:32:55.861963900Z"
    }
   },
   "outputs": [],
   "source": [
    "paddedImages = [f for f in listdir(\"data_padded\") if isfile(join(\"data_padded\", f))]\n",
    "trainData = list(K.utils.img_to_array(Image.open('data_padded/'+paddedImages[i])) for i in trainIndex)\n",
    "for i in range(len(trainData)):\n",
    "    trainData[i] = (trainData[i]-np.mean(trainData[i]))/np.std(trainData[i])\n",
    "trainData = np.array(trainData)\n",
    "labelData = np.array(labels.malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:33:06.832835600Z",
     "start_time": "2023-06-11T20:33:05.033675900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 resnet50 - False\n",
      "1 flatten_1 - True\n",
      "2 dense_1 - True\n"
     ]
    }
   ],
   "source": [
    "resmodel = applications.ResNet50(include_top=False, weights='imagenet', input_shape=(imageSize[0], imageSize[1], 3))\n",
    "\n",
    "model = K.models.Sequential()\n",
    "model.add(resmodel)\n",
    "model.add(K.layers.Flatten())\n",
    "model.add(K.layers.Dense(3, activation='softmax'))\n",
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, layer.name, \"-\", layer.trainable)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-11T20:41:36.730564200Z",
     "start_time": "2023-06-11T20:33:08.412413900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 41s 10s/step - loss: 53.6399 - accuracy: 0.4194\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 46s 11s/step - loss: 50.3556 - accuracy: 0.5806\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 48s 12s/step - loss: 17.4933 - accuracy: 0.2903\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 49s 12s/step - loss: 15.6979 - accuracy: 0.7097\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 49s 12s/step - loss: 10.8232 - accuracy: 0.6935\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 53s 13s/step - loss: 6.6398 - accuracy: 0.7419\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 52s 13s/step - loss: 6.8303 - accuracy: 0.7903\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 60s 15s/step - loss: 1.4998 - accuracy: 0.8871\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 56s 13s/step - loss: 1.6104 - accuracy: 0.8548\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 53s 13s/step - loss: 1.1451 - accuracy: 0.9355\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 28, 28, 2048)      23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1605632)           0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 4816899   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,404,611\n",
      "Trainable params: 4,816,899\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainData,labelData , verbose = 1, batch_size = 16, epochs = 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
